# Model Configuration for Sports Domain LLM

# Model Size Presets
# Choose one of: small, medium, large

# Small (~125M parameters)
small:
  vocab_size: 32000
  hidden_size: 768
  intermediate_size: 2048
  num_hidden_layers: 12
  num_attention_heads: 12
  num_key_value_heads: 12  # Same as attention heads (MHA)
  max_position_embeddings: 2048
  rms_norm_eps: 1.0e-6
  rope_theta: 10000.0
  use_flash_attention: true

# Medium (~350M parameters)
medium:
  vocab_size: 32000
  hidden_size: 1024
  intermediate_size: 2816
  num_hidden_layers: 24
  num_attention_heads: 16
  num_key_value_heads: 8  # GQA with 2 groups
  max_position_embeddings: 2048
  rms_norm_eps: 1.0e-6
  rope_theta: 10000.0
  use_flash_attention: true

# Large (~760M parameters)
large:
  vocab_size: 32000
  hidden_size: 1536
  intermediate_size: 4096
  num_hidden_layers: 24
  num_attention_heads: 16
  num_key_value_heads: 4  # GQA with 4 groups
  max_position_embeddings: 4096
  rms_norm_eps: 1.0e-6
  rope_theta: 10000.0
  use_flash_attention: true

# Special Tokens (shared across all sizes)
special_tokens:
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2

# Regularization (shared across all sizes)
regularization:
  hidden_dropout_prob: 0.0
  attention_dropout_prob: 0.0
  initializer_range: 0.02
