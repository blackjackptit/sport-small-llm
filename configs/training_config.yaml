# Training Configuration for Sports Domain LLM (From Scratch)

# Pretraining Configuration
pretraining:
  # Training
  num_epochs: 1
  max_steps: null  # Set to limit training steps
  batch_size: 8
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0

  # Learning Rate
  learning_rate: 3.0e-4
  min_learning_rate: 3.0e-5
  warmup_steps: 2000
  lr_scheduler: "cosine"  # "cosine" or "linear"

  # Optimizer
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95

  # Mixed Precision
  use_amp: true
  dtype: "bfloat16"  # "float16" or "bfloat16"

  # Checkpointing
  output_dir: "./outputs/pretrain"
  save_steps: 1000
  save_total_limit: 3

  # Logging
  logging_steps: 10
  eval_steps: 500
  wandb_project: "sports-llm-pretrain"

# Fine-tuning Configuration
finetuning:
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 8
  max_grad_norm: 1.0

  learning_rate: 2.0e-5
  warmup_ratio: 0.03
  weight_decay: 0.01

  use_amp: true
  dtype: "bfloat16"

  output_dir: "./outputs/finetune"
  save_steps: 500

  logging_steps: 10
  eval_steps: 100
  wandb_project: "sports-llm-sft"

# Data Configuration
data:
  train_dir: "data/processed"
  eval_file: null
  max_samples: null
  num_workers: 4
