{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sports Domain LLM - SageMaker Training\n",
    "\n",
    "This notebook guides you through training the Sports Domain LLM on AWS SageMaker.\n",
    "\n",
    "## Prerequisites\n",
    "- AWS account with SageMaker access\n",
    "- S3 bucket for data storage\n",
    "- SageMaker execution role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install sagemaker boto3 tokenizers torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Get SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "S3_BUCKET = sagemaker_session.default_bucket()  # Or use your own bucket\n",
    "S3_PREFIX = \"sports-llm\"\n",
    "\n",
    "# Model configuration\n",
    "MODEL_SIZE = \"small\"  # small, medium, or large\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "# Training configuration\n",
    "INSTANCE_TYPE = \"ml.g5.2xlarge\"  # Single A10G GPU\n",
    "INSTANCE_COUNT = 1\n",
    "\n",
    "print(f\"S3 Bucket: {S3_BUCKET}\")\n",
    "print(f\"S3 Prefix: {S3_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare and Upload Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample training data (for testing)\n",
    "sample_data = '''Lionel Messi led Argentina to victory in the 2022 FIFA World Cup.\n",
    "LeBron James has established himself as one of the greatest basketball players in NBA history.\n",
    "The New York Yankees hold the record for the most World Series championships with 27 titles.\n",
    "Tom Brady retired after winning seven Super Bowl championships.\n",
    "Manchester City won the UEFA Champions League for the first time in 2023.\n",
    "Novak Djokovic has won 24 Grand Slam singles titles.\n",
    "The Golden State Warriors revolutionized basketball with their three-point shooting philosophy.\n",
    "Michael Jordan is widely considered the greatest basketball player of all time.\n",
    "'''\n",
    "\n",
    "# Save locally\n",
    "with open('/tmp/sports_data.txt', 'w') as f:\n",
    "    f.write(sample_data)\n",
    "\n",
    "print(\"Sample data created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training data to S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Upload data\n",
    "train_s3_path = f\"s3://{S3_BUCKET}/{S3_PREFIX}/data/train/sports_data.txt\"\n",
    "s3_client.upload_file('/tmp/sports_data.txt', S3_BUCKET, f\"{S3_PREFIX}/data/train/sports_data.txt\")\n",
    "print(f\"Training data uploaded to: {train_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and upload tokenizer\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
    "import json\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Train tokenizer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=8000,\n",
    "    min_frequency=1,\n",
    "    special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    ")\n",
    "tokenizer.train(['/tmp/sports_data.txt'], trainer)\n",
    "\n",
    "# Save tokenizer\n",
    "os.makedirs('/tmp/tokenizer', exist_ok=True)\n",
    "tokenizer.save('/tmp/tokenizer/tokenizer.json')\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    \"vocab_size\": 8000,\n",
    "    \"min_frequency\": 1,\n",
    "    \"lowercase\": False,\n",
    "    \"pad_token_id\": 0,\n",
    "    \"bos_token_id\": 1,\n",
    "    \"eos_token_id\": 2,\n",
    "    \"unk_token_id\": 3\n",
    "}\n",
    "with open('/tmp/tokenizer/config.json', 'w') as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "print(f\"Tokenizer vocab size: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tarball and upload tokenizer\n",
    "with tarfile.open('/tmp/tokenizer.tar.gz', 'w:gz') as tar:\n",
    "    tar.add('/tmp/tokenizer/tokenizer.json', arcname='tokenizer.json')\n",
    "    tar.add('/tmp/tokenizer/config.json', arcname='config.json')\n",
    "\n",
    "tokenizer_s3_path = f\"s3://{S3_BUCKET}/{S3_PREFIX}/tokenizer/tokenizer.tar.gz\"\n",
    "s3_client.upload_file('/tmp/tokenizer.tar.gz', S3_BUCKET, f\"{S3_PREFIX}/tokenizer/tokenizer.tar.gz\")\n",
    "print(f\"Tokenizer uploaded to: {tokenizer_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: In a real setup, you would upload the src/ directory\n",
    "# For SageMaker, the source code is typically included via source_dir parameter\n",
    "# or uploaded separately\n",
    "\n",
    "# List current directory structure\n",
    "import os\n",
    "print(\"Expected project structure:\")\n",
    "print(\"\"\"\n",
    "build-fresh-llm/\n",
    "├── sagemaker/\n",
    "│   └── train.py       # Entry point\n",
    "├── src/\n",
    "│   ├── models/\n",
    "│   ├── tokenizer/\n",
    "│   ├── data/\n",
    "│   └── training/\n",
    "└── configs/\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure and Launch Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hyperparameters = {\n",
    "    \"model-size\": MODEL_SIZE,\n",
    "    \"max-seq-length\": MAX_SEQ_LENGTH,\n",
    "    \"epochs\": 1,\n",
    "    \"max-steps\": 100,  # Quick test\n",
    "    \"batch-size\": 4,\n",
    "    \"gradient-accumulation-steps\": 4,\n",
    "    \"learning-rate\": 3e-4,\n",
    "    \"warmup-steps\": 10,\n",
    "    \"save-steps\": 50,\n",
    "    \"logging-steps\": 5,\n",
    "    \"use-amp\": \"\",  # Flag to enable AMP\n",
    "}\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "for k, v in hyperparameters.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"../sagemaker\",  # Path to sagemaker directory\n",
    "    role=role,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    instance_count=INSTANCE_COUNT,\n",
    "    volume_size=100,  # GB\n",
    "    framework_version=\"2.1.0\",\n",
    "    py_version=\"py310\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=f\"s3://{S3_BUCKET}/{S3_PREFIX}/output\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    max_run=3600 * 4,  # 4 hours max\n",
    "    environment={\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"max_split_size_mb:512\",\n",
    "    },\n",
    "    dependencies=[\"../src\", \"../configs\"],  # Include source code\n",
    ")\n",
    "\n",
    "print(f\"Estimator created with instance type: {INSTANCE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input channels\n",
    "inputs = {\n",
    "    \"train\": TrainingInput(\n",
    "        s3_data=f\"s3://{S3_BUCKET}/{S3_PREFIX}/data/train\",\n",
    "        content_type=\"text/plain\",\n",
    "    ),\n",
    "    \"tokenizer\": TrainingInput(\n",
    "        s3_data=f\"s3://{S3_BUCKET}/{S3_PREFIX}/tokenizer\",\n",
    "        content_type=\"application/x-tar\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"Input channels:\")\n",
    "for name, channel in inputs.items():\n",
    "    print(f\"  {name}: {channel.s3_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training job!\n",
    "print(\"Launching training job...\")\n",
    "print(f\"Instance: {INSTANCE_TYPE} x {INSTANCE_COUNT}\")\n",
    "print(f\"Model size: {MODEL_SIZE}\")\n",
    "print(\"\")\n",
    "\n",
    "estimator.fit(inputs, wait=True)  # Set wait=False to run asynchronously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Monitor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job info\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "print(f\"Training job name: {training_job_name}\")\n",
    "\n",
    "# Get model artifacts location\n",
    "model_data = estimator.model_data\n",
    "print(f\"Model artifacts: {model_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View CloudWatch logs\n",
    "# You can also view logs in the AWS Console:\n",
    "# https://console.aws.amazon.com/cloudwatch/home#logsV2:log-groups/log-group/$252Faws$252Fsagemaker$252FTrainingJobs\n",
    "\n",
    "print(f\"View logs at:\")\n",
    "print(f\"https://console.aws.amazon.com/sagemaker/home?region={region}#/jobs/{training_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model artifacts\n",
    "import os\n",
    "\n",
    "local_model_path = \"./outputs/sagemaker_model\"\n",
    "os.makedirs(local_model_path, exist_ok=True)\n",
    "\n",
    "# Download from S3\n",
    "!aws s3 cp {model_data} {local_model_path}/model.tar.gz\n",
    "!cd {local_model_path} && tar -xzf model.tar.gz\n",
    "\n",
    "print(f\"Model downloaded to: {local_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.models.transformer import SportsLLM\n",
    "from src.tokenizer.tokenizer import SportsTokenizer\n",
    "\n",
    "# Load model\n",
    "config = torch.load(f\"{local_model_path}/config.pt\")\n",
    "model = SportsLLM(config)\n",
    "model.load_state_dict(torch.load(f\"{local_model_path}/model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = SportsTokenizer.load(f\"{local_model_path}/tokenizer\")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "prompt = \"The NBA\"\n",
    "input_ids = torch.tensor([tokenizer.encode(prompt)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.8,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(generated[0].tolist())\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate costs per hour (USD, varies by region)\n",
    "COSTS = {\n",
    "    \"ml.g5.xlarge\": 1.41,\n",
    "    \"ml.g5.2xlarge\": 1.69,\n",
    "    \"ml.g5.4xlarge\": 2.27,\n",
    "    \"ml.g5.12xlarge\": 7.09,\n",
    "    \"ml.p4d.24xlarge\": 37.69,\n",
    "}\n",
    "\n",
    "hours = 1  # Estimated training time\n",
    "cost = COSTS.get(INSTANCE_TYPE, 0) * INSTANCE_COUNT * hours\n",
    "\n",
    "print(f\"Estimated cost for {hours} hour(s):\")\n",
    "print(f\"  Instance: {INSTANCE_TYPE} x {INSTANCE_COUNT}\")\n",
    "print(f\"  Cost: ${cost:.2f} USD\")\n",
    "print(f\"  With spot instances (~65% savings): ${cost * 0.35:.2f} USD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
